{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "#  Text Preprocessing for Financial Sentiment Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OMWlJ78nvZi"
      },
      "source": [
        "## 1. Setup and Library Imports <a class=\"anchor\" id=\"setup\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vjedOFyyk_Nm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-09 23:40:59,010 - INFO - NLTK stopwords downloaded.\n",
            "2025-07-09 23:40:59,014 - ERROR - Error loading SpaCy model: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.. Please run 'python -m spacy download en_core_web_sm'\n",
            "2025-07-09 23:41:00,071 - INFO - FinBERT tokenizer loaded.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import spacy\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import AutoTokenizer\n",
        "import logging\n",
        "\n",
        "# Configure logging for better visibility\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "try:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    logging.info(\"NLTK stopwords downloaded.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error downloading NLTK stopwords: {e}\")\n",
        "\n",
        "try:\n",
        "    # Load a smaller model for general text processing if 'en_core_web_sm' is too large\n",
        "    # For this specific task, if only tokenization is needed, spacy might be optional\n",
        "    spacy.load('en_core_web_sm')\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    logging.info(\"SpaCy 'en_core_web_sm' loaded.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading SpaCy model: {e}. Please run 'python -m spacy download en_core_web_sm'\")\n",
        "    nlp = None \n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "    logging.info(\"FinBERT tokenizer loaded.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading FinBERT tokenizer: {e}\")\n",
        "    tokenizer = None \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuVm05HtpTFR"
      },
      "source": [
        "## 2. Load Dataset <a class=\"anchor\" id=\"load_dataset\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "amBTH0J_pVDn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-09 23:41:00,097 - INFO - Dataset loaded successfully from C:\\Users\\teamp\\Desktop\\Financial Sentiment Analysis\\data\\raw\\financial_phrasebank.csv. Shape: (2264, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label\n",
              "0  According to Gran , the company has no plans t...      1\n",
              "1  For the last quarter of 2010 , Componenta 's n...      2\n",
              "2  In the third quarter of 2010 , net sales incre...      2\n",
              "3  Operating profit rose to EUR 13.1 mn from EUR ...      2\n",
              "4  Operating profit totalled EUR 21.1 mn , up fro...      2"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "file_path = r\"C:\\Users\\teamp\\Desktop\\Financial Sentiment Analysis\\data\\raw\\financial_phrasebank.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    logging.info(f\"Dataset loaded successfully from {file_path}. Shape: {df.shape}\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    logging.error(f\"Error: The file {file_path} was not found. Please upload it or check the path.\")\n",
        "    df = pd.DataFrame() \n",
        "except Exception as e:\n",
        "    logging.error(f\"An error occurred while loading the dataset: {e}\")\n",
        "    df = pd.DataFrame() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YLfEbe3qN6Y"
      },
      "source": [
        "## 3. Text Cleaning <a class=\"anchor\" id=\"text_cleaning\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tPBCCJOYqWoY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-09 23:41:00,137 - INFO - Applying text cleaning to the 'sentence' column...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>For the last quarter of 2010 , Componenta s ne...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  label\n",
              "0  According to Gran , the company has no plans t...      1\n",
              "1  For the last quarter of 2010 , Componenta s ne...      2\n",
              "2  In the third quarter of 2010 , net sales incre...      2\n",
              "3  Operating profit rose to EUR 13.1 mn from EUR ...      2\n",
              "4  Operating profit totalled EUR 21.1 mn , up fro...      2"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans a given text string by removing unwanted characters and normalizing whitespace.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text string.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text string.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        logging.warning(f\"Non-string input detected for cleaning: {type(text)}. Converting to string.\")\n",
        "        text = str(text)\n",
        "\n",
        "    # Keep important symbols, remove unwanted chars\n",
        "    text = re.sub(r\"[^a-zA-Z0-9$€%.,!? ]+\", \" \", text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "if not df.empty:\n",
        "    logging.info(\"Applying text cleaning to the 'sentence' column...\")\n",
        "    df['sentence'] = df['sentence'].apply(clean_text)\n",
        "    display(df.head())\n",
        "else:\n",
        "    logging.warning(\"DataFrame is empty. Skipping text cleaning.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y86j9B62vcQg"
      },
      "source": [
        "## 4. Tokenization <a class=\"anchor\" id=\"tokenization\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ua6LIALrucL3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-09 23:41:00,225 - INFO - Tokenizing the dataset...\n",
            "2025-07-09 23:41:00,409 - INFO - Tokens shape: torch.Size([2264, 64])\n"
          ]
        }
      ],
      "source": [
        "if tokenizer and not df.empty:\n",
        "    logging.info(\"Tokenizing the dataset...\")\n",
        "    try:\n",
        "        tokens = tokenizer(\n",
        "            list(df['sentence']),\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=64,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        logging.info(f\"Tokens shape: {tokens['input_ids'].shape}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during tokenization: {e}\")\n",
        "        tokens = None\n",
        "else:\n",
        "    logging.warning(\"Tokenizer not loaded or DataFrame is empty. Skipping tokenization.\")\n",
        "    tokens = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd0_JaoWwq8f"
      },
      "source": [
        "## 5. Prepare Labels Tensor <a class=\"anchor\" id=\"prepare_labels\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "PqyE_JyQurDI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-09 23:41:00,432 - INFO - Preparing labels tensor...\n",
            "2025-07-09 23:41:00,433 - INFO - Labels tensor shape: torch.Size([2264])\n"
          ]
        }
      ],
      "source": [
        "if not df.empty:\n",
        "    logging.info(\"Preparing labels tensor...\")\n",
        "    try:\n",
        "        # Ensure labels are numeric based on the map if they are not already\n",
        "        # Assuming 'label' column already contains numeric labels (0, 1, 2) as per df.head()\n",
        "        labels = torch.tensor(df['label'].values, dtype=torch.long)\n",
        "        logging.info(f\"Labels tensor shape: {labels.shape}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error preparing labels tensor: {e}\")\n",
        "        labels = None\n",
        "else:\n",
        "    logging.warning(\"DataFrame is empty. Skipping label preparation.\")\n",
        "    labels = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved successfully at: C:\\Users\\teamp\\Desktop\\Financial Sentiment Analysis\\data\\processed\\cleaned_data.csv\n"
          ]
        }
      ],
      "source": [
        "output_path = r\"C:\\Users\\teamp\\Desktop\\Financial Sentiment Analysis\\data\\processed\\cleaned_data.csv\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "import os\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "# Save DataFrame to CSV without the index\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Data saved successfully at: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Null or missing texts: 0\n",
            "Empty or whitespace-only texts: 0\n",
            "Texts containing special characters: 2259\n",
            "Sample texts:\n",
            "1: According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .\n",
            "2: For the last quarter of 2010 , Componenta s net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre tax profit from a pre tax loss of EUR7m .\n",
            "3: In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .\n",
            "4: Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .\n",
            "5: Operating profit totalled EUR 21.1 mn , up from EUR 18.6 mn in 2007 , representing 9.7 % of net sales .\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def check_nlp_data_cleanliness(df, text_col):\n",
        "    # Verify the text column exists\n",
        "    if text_col not in df.columns:\n",
        "        print(f\"Error: Column '{text_col}' not found in DataFrame.\")\n",
        "        print(f\"Available columns: {df.columns.tolist()}\")\n",
        "        return\n",
        "    \n",
        "    # 1. Null or missing values\n",
        "    null_count = df[text_col].isnull().sum()\n",
        "\n",
        "    # 2. Empty or whitespace-only strings\n",
        "    empty_count = (df[text_col].astype(str).str.strip() == '').sum()\n",
        "\n",
        "    # 3. Special characters count\n",
        "    def contains_special_chars(text):\n",
        "        if not isinstance(text, str):\n",
        "            return False\n",
        "        return bool(re.search(r'[^a-zA-Z0-9\\s]', text))\n",
        "\n",
        "    special_char_count = df[text_col].apply(contains_special_chars).sum()\n",
        "\n",
        "    # 4. Sample few texts for manual inspection\n",
        "    sample_texts = df[text_col].head(5).tolist()\n",
        "\n",
        "    # Results printout\n",
        "    print(f\"Null or missing texts: {null_count}\")\n",
        "    print(f\"Empty or whitespace-only texts: {empty_count}\")\n",
        "    print(f\"Texts containing special characters: {special_char_count}\")\n",
        "    print(\"Sample texts:\")\n",
        "    for i, text in enumerate(sample_texts, 1):\n",
        "        print(f\"{i}: {text}\")\n",
        "\n",
        "# Usage example:\n",
        "text_column = 'sentence'  # your actual text column name from the DataFrame\n",
        "check_nlp_data_cleanliness(df, text_column)\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "authorship_tag": "ABX9TyMxIp15DaRLJir5AOGJbcYj",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
